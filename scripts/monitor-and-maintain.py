#!/usr/bin/env python3
"""
Excel MCP Server - ç›‘æ§å’Œç»´æŠ¤è„šæœ¬

è¯¥è„šæœ¬æä¾›å…¨é¢çš„ç›‘æ§å’Œç»´æŠ¤åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. è¦†ç›–ç‡ç›‘æ§ - æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡æ˜¯å¦è¾¾åˆ°è¦æ±‚
2. æµ‹è¯•è¿è¡Œæ—¶é—´ç›‘æ§ - ç›‘æ§æµ‹è¯•æ‰§è¡Œæ—¶é—´å’Œæ€§èƒ½
3. å†…å­˜ä½¿ç”¨ç›‘æ§ - æ£€æŸ¥æµ‹è¯•è¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
4. æµ‹è¯•è´¨é‡è¯„ä¼° - è¯„ä¼°æµ‹è¯•è´¨é‡å’Œç¨³å®šæ€§
5. è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ - ç”Ÿæˆè¯¦ç»†çš„ç›‘æ§æŠ¥å‘Š
6. ç»´æŠ¤å»ºè®® - æä¾›æ”¹è¿›å»ºè®®

ä½¿ç”¨æ–¹æ³•:
    python scripts/monitor-and-maintain.py [é€‰é¡¹]

é€‰é¡¹:
    --coverage-only     ä»…è¿è¡Œè¦†ç›–ç‡ç›‘æ§
    --performance-only  ä»…è¿è¡Œæ€§èƒ½ç›‘æ§
    --memory-only       ä»…è¿è¡Œå†…å­˜ç›‘æ§
    --quality-only      ä»…è¿è¡Œè´¨é‡è¯„ä¼°
    --report-file       æŒ‡å®šæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: reports/monitoring-report.html)
    --threshold         è¦†ç›–ç‡é˜ˆå€¼ (é»˜è®¤: 85)
    --no-html           ä¸ç”ŸæˆHTMLæŠ¥å‘Šï¼Œä»…è¾“å‡ºæ–‡æœ¬
    --continuous        è¿ç»­ç›‘æ§æ¨¡å¼ï¼Œæ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡
"""

import argparse
import json
import os
import psutil
import subprocess
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging
from dataclasses import dataclass, asdict
import threading
import signal
import tempfile

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class CoverageMetrics:
    """è¦†ç›–ç‡æŒ‡æ ‡"""
    total_coverage: float
    file_coverage: Dict[str, float]
    missing_lines: Dict[str, List[int]]
    uncovered_files: List[str]
    timestamp: datetime


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    total_time: float
    test_count: int
    success_rate: float
    slowest_tests: List[Tuple[str, float]]
    fastest_tests: List[Tuple[str, float]]
    memory_usage_mb: float
    timestamp: datetime


@dataclass
class MemoryMetrics:
    """å†…å­˜ä½¿ç”¨æŒ‡æ ‡"""
    peak_memory_mb: float
    average_memory_mb: float
    memory_growth_mb: float
    process_count: int
    timestamp: datetime


@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    test_stability: float
    flaky_tests: List[str]
    code_quality_score: float
    duplicate_coverage: float
    test_complexity_score: float
    timestamp: datetime


@dataclass
class MonitoringReport:
    """ç›‘æ§æŠ¥å‘Š"""
    timestamp: datetime
    coverage: CoverageMetrics
    performance: PerformanceMetrics
    memory: MemoryMetrics
    quality: QualityMetrics
    recommendations: List[str]
    summary: Dict[str, Any]


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.measurements = []
        self.monitoring = False
        self.monitor_thread = None
        self.start_time = None

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨"""
        self.monitoring = True
        self.start_time = time.time()
        self.measurements = []

        def monitor():
            process = psutil.Process()
            while self.monitoring:
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.measurements.append({
                    'timestamp': time.time() - self.start_time,
                    'memory_mb': memory_mb
                })
                time.sleep(0.5)  # æ¯0.5ç§’é‡‡é›†ä¸€æ¬¡

        self.monitor_thread = threading.Thread(target=monitor)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        logger.info("å†…å­˜ç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self) -> MemoryMetrics:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›å†…å­˜æŒ‡æ ‡"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()

        if not self.measurements:
            return MemoryMetrics(0, 0, 0, 0, datetime.now())

        memory_values = [m['memory_mb'] for m in self.measurements]
        peak_memory = max(memory_values)
        average_memory = sum(memory_values) / len(memory_values)

        # è®¡ç®—å†…å­˜å¢é•¿
        if len(self.measurements) >= 2:
            initial_memory = self.measurements[0]['memory_mb']
            final_memory = self.measurements[-1]['memory_mb']
            memory_growth = final_memory - initial_memory
        else:
            memory_growth = 0

        logger.info(f"å†…å­˜ç›‘æ§ç»“æŸ - å³°å€¼: {peak_memory:.1f}MB, å¹³å‡: {average_memory:.1f}MB, å¢é•¿: {memory_growth:.1f}MB")

        return MemoryMetrics(
            peak_memory_mb=peak_memory,
            average_memory_mb=average_memory,
            memory_growth_mb=memory_growth,
            process_count=len(self.measurements),
            timestamp=datetime.now()
        )


class TestMonitor:
    """æµ‹è¯•ç›‘æ§å™¨"""

    def __init__(self, coverage_threshold: float = 85.0):
        self.coverage_threshold = coverage_threshold
        self.base_dir = Path(__file__).parent.parent
        self.reports_dir = self.base_dir / "reports"
        self.logs_dir = self.base_dir / "logs"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.reports_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)

    def run_coverage_monitoring(self) -> CoverageMetrics:
        """è¿è¡Œè¦†ç›–ç‡ç›‘æ§"""
        logger.info("å¼€å§‹è¦†ç›–ç‡ç›‘æ§...")

        try:
            # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/",
                "--cov=src",
                "--cov-report=json",
                "--cov-report=term-missing",
                "--cov-report=html:htmlcov",
                "-v"
            ]

            result = subprocess.run(
                cmd,
                cwd=self.base_dir,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )

            # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
            coverage_file = self.base_dir / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file, 'r', encoding='utf-8') as f:
                    coverage_data = json.load(f)

                total_coverage = coverage_data['totals']['percent_covered']
                file_coverage = {}
                missing_lines = {}
                uncovered_files = []

                for filename, file_data in coverage_data['files'].items():
                    file_coverage[filename] = file_data['summary']['percent_covered']

                    if file_data['summary']['missing_lines']:
                        missing_lines[filename] = file_data['summary']['missing_lines']

                    if file_data['summary']['percent_covered'] == 0:
                        uncovered_files.append(filename)

                logger.info(f"æ€»è¦†ç›–ç‡: {total_coverage:.1f}%")

                return CoverageMetrics(
                    total_coverage=total_coverage,
                    file_coverage=file_coverage,
                    missing_lines=missing_lines,
                    uncovered_files=uncovered_files,
                    timestamp=datetime.now()
                )
            else:
                logger.warning("æœªæ‰¾åˆ°è¦†ç›–ç‡æŠ¥å‘Šæ–‡ä»¶")
                return CoverageMetrics(0, {}, {}, [], datetime.now())

        except subprocess.TimeoutExpired:
            logger.error("è¦†ç›–ç‡æµ‹è¯•è¶…æ—¶")
            return CoverageMetrics(0, {}, {}, [], datetime.now())
        except Exception as e:
            logger.error(f"è¦†ç›–ç‡ç›‘æ§å¤±è´¥: {e}")
            return CoverageMetrics(0, {}, {}, [], datetime.now())

    def run_performance_monitoring(self, memory_monitor: MemoryMonitor) -> PerformanceMetrics:
        """è¿è¡Œæ€§èƒ½ç›‘æ§"""
        logger.info("å¼€å§‹æ€§èƒ½ç›‘æ§...")

        start_time = time.time()

        try:
            # å¯åŠ¨å†…å­˜ç›‘æ§
            memory_monitor.start_monitoring()

            # è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†æ€§èƒ½æ•°æ®
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/",
                "--tb=short",
                "-v",
                "--durations=10"  # æ˜¾ç¤ºæœ€æ…¢çš„10ä¸ªæµ‹è¯•
            ]

            result = subprocess.run(
                cmd,
                cwd=self.base_dir,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            total_time = time.time() - start_time

            # è§£ææµ‹è¯•ç»“æœ
            output_lines = result.stdout.split('\n')
            test_count = 0
            failed_count = 0
            slow_tests = []

            for line in output_lines:
                if "passed" in line and "failed" in line:
                    # æå–æµ‹è¯•æ•°é‡
                    import re
                    match = re.search(r'(\d+)\s+passed.*?(\d+)\s+failed', line)
                    if match:
                        test_count = int(match.group(1))
                        failed_count = int(match.group(2))
                elif "seconds" in line and "::" in line:
                    # æå–æ…¢é€Ÿæµ‹è¯•
                    parts = line.split()
                    if len(parts) >= 2:
                        test_name = parts[-1]
                        duration = float(parts[0])
                        slow_tests.append((test_name, duration))

            success_rate = (test_count - failed_count) / max(test_count, 1) * 100

            # åœæ­¢å†…å­˜ç›‘æ§
            memory_metrics = memory_monitor.stop_monitoring()

            logger.info(f"æ€§èƒ½ç›‘æ§å®Œæˆ - æ€»æ—¶é—´: {total_time:.1f}s, æµ‹è¯•æ•°: {test_count}, æˆåŠŸç‡: {success_rate:.1f}%")

            return PerformanceMetrics(
                total_time=total_time,
                test_count=test_count,
                success_rate=success_rate,
                slowest_tests=sorted(slow_tests, key=lambda x: x[1], reverse=True)[:5],
                fastest_tests=sorted(slow_tests, key=lambda x: x[1])[:5],
                memory_usage_mb=memory_metrics.peak_memory_mb,
                timestamp=datetime.now()
            )

        except subprocess.TimeoutExpired:
            memory_monitor.stop_monitoring()
            logger.error("æ€§èƒ½æµ‹è¯•è¶…æ—¶")
            return PerformanceMetrics(600, 0, 0, [], [], 0, datetime.now())
        except Exception as e:
            memory_monitor.stop_monitoring()
            logger.error(f"æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            return PerformanceMetrics(0, 0, 0, [], [], 0, datetime.now())

    def run_quality_assessment(self, coverage_metrics: CoverageMetrics,
                             performance_metrics: PerformanceMetrics) -> QualityMetrics:
        """è¿è¡Œè´¨é‡è¯„ä¼°"""
        logger.info("å¼€å§‹è´¨é‡è¯„ä¼°...")

        try:
            # è®¡ç®—æµ‹è¯•ç¨³å®šæ€§
            test_stability = self._calculate_test_stability()

            # æ£€æµ‹flakyæµ‹è¯•
            flaky_tests = self._detect_flaky_tests()

            # è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°
            code_quality_score = self._calculate_code_quality_score(coverage_metrics)

            # è®¡ç®—é‡å¤è¦†ç›–ç‡
            duplicate_coverage = self._calculate_duplicate_coverage()

            # è®¡ç®—æµ‹è¯•å¤æ‚åº¦åˆ†æ•°
            test_complexity_score = self._calculate_test_complexity()

            logger.info(f"è´¨é‡è¯„ä¼°å®Œæˆ - ç¨³å®šæ€§: {test_stability:.1f}, è´¨é‡åˆ†æ•°: {code_quality_score:.1f}")

            return QualityMetrics(
                test_stability=test_stability,
                flaky_tests=flaky_tests,
                code_quality_score=code_quality_score,
                duplicate_coverage=duplicate_coverage,
                test_complexity_score=test_complexity_score,
                timestamp=datetime.now()
            )

        except Exception as e:
            logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return QualityMetrics(0, [], 0, 0, 0, datetime.now())

    def _calculate_test_stability(self) -> float:
        """è®¡ç®—æµ‹è¯•ç¨³å®šæ€§"""
        try:
            # è¿è¡Œæµ‹è¯•å¤šæ¬¡æ¥æ£€æµ‹ä¸ç¨³å®šçš„æµ‹è¯•
            stability_scores = []

            for i in range(3):  # è¿è¡Œ3æ¬¡
                cmd = [sys.executable, "-m", "pytest", "tests/", "-q"]
                result = subprocess.run(cmd, cwd=self.base_dir, capture_output=True)

                # ç»Ÿè®¡é€šè¿‡çš„æµ‹è¯•æ•°é‡
                if result.returncode == 0:
                    output = result.stdout.decode('utf-8', errors='ignore')
                    import re
                    match = re.search(r'(\d+)\s+passed', output)
                    if match:
                        stability_scores.append(int(match.group(1)))

            if len(stability_scores) > 1:
                # è®¡ç®—æ ‡å‡†å·®
                mean = sum(stability_scores) / len(stability_scores)
                variance = sum((x - mean) ** 2 for x in stability_scores) / len(stability_scores)
                std_dev = variance ** 0.5

                # ç¨³å®šæ€§åˆ†æ•° (æ ‡å‡†å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜)
                stability = max(0, 100 - std_dev)
                return stability
            else:
                return 100.0  # æ— æ³•è®¡ç®—ç¨³å®šæ€§ï¼Œå‡è®¾ç¨³å®š

        except Exception as e:
            logger.warning(f"æµ‹è¯•ç¨³å®šæ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _detect_flaky_tests(self) -> List[str]:
        """æ£€æµ‹ä¸ç¨³å®šçš„æµ‹è¯•"""
        flaky_tests = []

        try:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„flakyæµ‹è¯•æ£€æµ‹é€»è¾‘
            # æ¯”å¦‚è¿è¡Œå¤šæ¬¡å¹¶æ¯”è¾ƒç»“æœ
            test_results = {}

            for i in range(3):
                cmd = [sys.executable, "-m", "pytest", "tests/", "--tb=no", "-v"]
                result = subprocess.run(cmd, cwd=self.base_dir, capture_output=True)

                if result.returncode != 0:
                    # è§£æå¤±è´¥çš„æµ‹è¯•
                    output = result.stderr.decode('utf-8', errors='ignore')
                    lines = output.split('\n')
                    for line in lines:
                        if '::' in line and 'FAILED' in line:
                            test_name = line.split('FAILED')[0].strip()
                            test_results[test_name] = test_results.get(test_name, 0) + 1

            # æ‰¾å‡ºå¤±è´¥çš„æµ‹è¯•
            flaky_tests = [test for test, count in test_results.items() if count > 1]

        except Exception as e:
            logger.warning(f"Flakyæµ‹è¯•æ£€æµ‹å¤±è´¥: {e}")

        return flaky_tests

    def _calculate_code_quality_score(self, coverage_metrics: CoverageMetrics) -> float:
        """è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°"""
        score = 0.0

        # è¦†ç›–ç‡åˆ†æ•° (40%)
        coverage_score = min(100, coverage_metrics.total_coverage * 100 / self.coverage_threshold)
        score += coverage_score * 0.4

        # æ–‡ä»¶è¦†ç›–ç‡ä¸€è‡´æ€§åˆ†æ•° (20%)
        if coverage_metrics.file_coverage:
            coverages = list(coverage_metrics.file_coverage.values())
            avg_coverage = sum(coverages) / len(coverages)
            consistency = 100 - (max(coverages) - min(coverages))
            score += min(100, consistency) * 0.2

        # æ— æœªè¦†ç›–æ–‡ä»¶åˆ†æ•° (20%)
        uncovered_penalty = len(coverage_metrics.uncovered_files) * 10
        uncovered_score = max(0, 100 - uncovered_penalty)
        score += uncovered_score * 0.2

        # åŸºç¡€åˆ†æ•° (20%)
        score += 20

        return min(100, score)

    def _calculate_duplicate_coverage(self) -> float:
        """è®¡ç®—é‡å¤è¦†ç›–ç‡"""
        try:
            # åˆ†ææµ‹è¯•ç”¨ä¾‹çš„é‡å¤æ€§
            cmd = [sys.executable, "-m", "pytest", "tests/", "--collect-only", "-q"]
            result = subprocess.run(cmd, cwd=self.base_dir, capture_output=True, text=True)

            if result.returncode == 0:
                output = result.stdout
                test_names = [line.strip() for line in output.split('\n') if '::test_' in line]

                # ç®€å•çš„é‡å¤æ£€æµ‹ï¼šæŸ¥æ‰¾ç›¸ä¼¼çš„æµ‹è¯•åç§°
                duplicate_count = 0
                for i, test1 in enumerate(test_names):
                    for test2 in test_names[i+1:]:
                        # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æµ‹
                        if test1.split('::')[-1].split('_')[0] == test2.split('::')[-1].split('_')[0]:
                            duplicate_count += 1

                if len(test_names) > 0:
                    duplicate_percentage = (duplicate_count / len(test_names)) * 100
                    return duplicate_percentage

            return 0.0

        except Exception as e:
            logger.warning(f"é‡å¤è¦†ç›–ç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _calculate_test_complexity(self) -> float:
        """è®¡ç®—æµ‹è¯•å¤æ‚åº¦åˆ†æ•°"""
        try:
            complexity_scores = []

            # åˆ†ææµ‹è¯•æ–‡ä»¶çš„å¤æ‚åº¦
            test_files = list(Path(self.base_dir / "tests").glob("test_*.py"))

            for test_file in test_files:
                try:
                    with open(test_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ç®€å•çš„å¤æ‚åº¦æŒ‡æ ‡
                    lines = len(content.split('\n'))
                    functions = content.count('def test_')
                    asserts = content.count('assert')

                    # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
                    if functions > 0:
                        avg_lines_per_test = lines / functions
                        asserts_per_test = asserts / functions

                        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªæµ‹è¯•10-20è¡Œï¼ŒåŒ…å«1-3ä¸ªæ–­è¨€
                        length_score = max(0, 100 - abs(avg_lines_per_test - 15) * 2)
                        assert_score = max(0, 100 - abs(asserts_per_test - 2) * 20)

                        test_score = (length_score + assert_score) / 2
                        complexity_scores.append(test_score)

                except Exception:
                    continue

            if complexity_scores:
                return sum(complexity_scores) / len(complexity_scores)
            else:
                return 50.0  # é»˜è®¤åˆ†æ•°

        except Exception as e:
            logger.warning(f"æµ‹è¯•å¤æ‚åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def generate_recommendations(self, report: MonitoringReport) -> List[str]:
        """ç”Ÿæˆç»´æŠ¤å»ºè®®"""
        recommendations = []

        # è¦†ç›–ç‡å»ºè®®
        if report.coverage.total_coverage < self.coverage_threshold:
            recommendations.append(
                f"âš ï¸ æµ‹è¯•è¦†ç›–ç‡ ({report.coverage.total_coverage:.1f}%) ä½äºè¦æ±‚ ({self.coverage_threshold}%)ï¼Œ"
                f"å»ºè®®å¢åŠ æµ‹è¯•ç”¨ä¾‹ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹æ–‡ä»¶ï¼š{', '.join(report.coverage.uncovered_files[:3])}"
            )

        # æ€§èƒ½å»ºè®®
        if report.performance.total_time > 300:  # 5åˆ†é’Ÿ
            recommendations.append(
                f"â±ï¸ æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({report.performance.total_time:.1f}s)ï¼Œ"
                f"å»ºè®®ä¼˜åŒ–ä»¥ä¸‹æ…¢é€Ÿæµ‹è¯•ï¼š{', '.join([name for name, _ in report.performance.slowest_tests[:3]])}"
            )

        if report.performance.memory_usage_mb > 1000:  # 1GB
            recommendations.append(
                f"ğŸ’¾ å†…å­˜ä½¿ç”¨è¿‡é«˜ ({report.performance.memory_usage_mb:.1f}MB)ï¼Œ"
                "å»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼æˆ–ä¼˜åŒ–æµ‹è¯•æ•°æ®ç®¡ç†"
            )

        # è´¨é‡å»ºè®®
        if report.quality.test_stability < 90:
            recommendations.append(
                f"ğŸ”„ æµ‹è¯•ç¨³å®šæ€§è¾ƒä½ ({report.quality.test_stability:.1f}%)ï¼Œ"
                f"éœ€è¦ä¿®å¤ä»¥ä¸‹ä¸ç¨³å®šçš„æµ‹è¯•ï¼š{', '.join(report.quality.flaky_tests[:3])}"
            )

        if report.quality.code_quality_score < 80:
            recommendations.append(
                f"ğŸ“Š ä»£ç è´¨é‡åˆ†æ•°è¾ƒä½ ({report.quality.code_quality_score:.1f}%)ï¼Œ"
                "å»ºè®®æ”¹è¿›æµ‹è¯•ç»“æ„å’Œè¦†ç›–ç‡åˆ†å¸ƒ"
            )

        if report.quality.duplicate_coverage > 20:
            recommendations.append(
                f"ğŸ”„ å‘ç°é‡å¤æµ‹è¯•ç”¨ä¾‹ ({report.quality.duplicate_coverage:.1f}%)ï¼Œ"
                "å»ºè®®é‡æ„æµ‹è¯•ä»¥å‡å°‘é‡å¤"
            )

        if not recommendations:
            recommendations.append("âœ… æ‰€æœ‰æŒ‡æ ‡éƒ½åœ¨è‰¯å¥½èŒƒå›´å†…ï¼Œç»§ç»­ä¿æŒï¼")

        return recommendations

    def generate_report(self, metrics: Dict[str, Any], output_file: str = None) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç›‘æ§æŠ¥å‘Š...")

        # åˆ›å»ºæŠ¥å‘Šæ•°æ®
        coverage = metrics['coverage']
        performance = metrics['performance']
        memory = metrics['memory']
        quality = metrics['quality']

        report = MonitoringReport(
            timestamp=datetime.now(),
            coverage=coverage,
            performance=performance,
            memory=memory,
            quality=quality,
            recommendations=metrics.get('recommendations', []),
            summary={
                'overall_score': self._calculate_overall_score(coverage, performance, memory, quality),
                'status': self._get_overall_status(coverage, performance, memory, quality),
                'key_metrics': {
                    'coverage': coverage.total_coverage,
                    'test_count': performance.test_count,
                    'success_rate': performance.success_rate,
                    'memory_peak': memory.peak_memory_mb,
                    'quality_score': quality.code_quality_score
                }
            }
        )

        # ç”ŸæˆHTMLæŠ¥å‘Š
        if output_file and not output_file.endswith('.txt'):
            html_content = self._generate_html_report(report)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_report = self._generate_text_report(report)

        # å¦‚æœæ˜¯è¿ç»­ç›‘æ§æ¨¡å¼ï¼Œè¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶
        if hasattr(self, '_continuous_mode') and self._continuous_mode:
            log_file = self.logs_dir / "continuous_monitor.log"
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{'='*60}\n{datetime.now().isoformat()}\n{'='*60}\n")
                f.write(text_report)

        return text_report

    def _calculate_overall_score(self, coverage, performance, memory, quality) -> float:
        """è®¡ç®—æ€»ä½“åˆ†æ•°"""
        score = 0.0

        # è¦†ç›–ç‡åˆ†æ•° (30%)
        coverage_score = min(100, coverage.total_coverage)
        score += coverage_score * 0.3

        # æ€§èƒ½åˆ†æ•° (25%)
        performance_score = 0
        if performance.success_rate > 95:
            performance_score += 50
        if performance.total_time < 180:  # 3åˆ†é’Ÿ
            performance_score += 30
        if performance.memory_usage_mb < 500:  # 500MB
            performance_score += 20
        score += min(100, performance_score) * 0.25

        # å†…å­˜åˆ†æ•° (20%)
        memory_score = 0
        if memory.peak_memory_mb < 500:
            memory_score = 100
        elif memory.peak_memory_mb < 1000:
            memory_score = 80
        elif memory.peak_memory_mb < 1500:
            memory_score = 60
        else:
            memory_score = 40
        score += memory_score * 0.2

        # è´¨é‡åˆ†æ•° (25%)
        score += quality.code_quality_score * 0.25

        return min(100, score)

    def _get_overall_status(self, coverage, performance, memory, quality) -> str:
        """è·å–æ€»ä½“çŠ¶æ€"""
        overall_score = self._calculate_overall_score(coverage, performance, memory, quality)

        if overall_score >= 90:
            return "ä¼˜ç§€"
        elif overall_score >= 80:
            return "è‰¯å¥½"
        elif overall_score >= 70:
            return "ä¸€èˆ¬"
        elif overall_score >= 60:
            return "éœ€è¦æ”¹è¿›"
        else:
            return "ä¸¥é‡é—®é¢˜"

    def _generate_html_report(self, report: MonitoringReport) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_template = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excel MCP Server - ç›‘æ§æŠ¥å‘Š</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); overflow: hidden; }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }
        .header h1 { margin: 0; font-size: 2.5em; }
        .header .timestamp { opacity: 0.9; margin-top: 10px; }
        .status { display: inline-block; padding: 8px 16px; border-radius: 20px; margin-top: 15px; font-weight: bold; }
        .status.ä¼˜ç§€ { background: #4caf50; }
        .status.è‰¯å¥½ { background: #8bc34a; }
        .status.ä¸€èˆ¬ { background: #ff9800; }
        .status.éœ€è¦æ”¹è¿› { background: #f44336; }
        .status.ä¸¥é‡é—®é¢˜ { background: #d32f2f; }
        .content { padding: 30px; }
        .section { margin-bottom: 40px; }
        .section h2 { color: #333; border-bottom: 2px solid #667eea; padding-bottom: 10px; margin-bottom: 20px; }
        .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-bottom: 30px; }
        .metric-card { background: #f8f9fa; border-radius: 8px; padding: 20px; border-left: 4px solid #667eea; }
        .metric-card h3 { margin: 0 0 15px 0; color: #495057; }
        .metric-value { font-size: 2em; font-weight: bold; color: #667eea; margin-bottom: 5px; }
        .metric-label { color: #6c757d; font-size: 0.9em; }
        .progress-bar { background: #e9ecef; border-radius: 4px; height: 8px; overflow: hidden; margin-top: 10px; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, #667eea, #764ba2); transition: width 0.3s ease; }
        .recommendations { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 20px; }
        .recommendations h3 { color: #856404; margin-top: 0; }
        .recommendations ul { margin: 10px 0; padding-left: 20px; }
        .recommendations li { margin-bottom: 8px; color: #856404; }
        .table { width: 100%; border-collapse: collapse; margin-top: 15px; }
        .table th, .table td { padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }
        .table th { background: #f8f9fa; font-weight: 600; color: #495057; }
        .chart-placeholder { background: #f8f9fa; border: 2px dashed #dee2e6; border-radius: 8px; padding: 40px; text-align: center; color: #6c757d; }
        .footer { background: #f8f9fa; padding: 20px; text-align: center; color: #6c757d; font-size: 0.9em; }
        .score-circle { width: 120px; height: 120px; border-radius: 50%; background: conic-gradient(#667eea 0deg, #667eea {score}deg, #e9ecef {score}deg); display: flex; align-items: center; justify-content: center; margin: 20px auto; position: relative; }
        .score-circle::before { content: ''; position: absolute; width: 100px; height: 100px; background: white; border-radius: 50%; }
        .score-text { position: relative; font-size: 1.8em; font-weight: bold; color: #667eea; }
        .good { color: #28a745; }
        .warning { color: #ffc107; }
        .bad { color: #dc3545; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ” Excel MCP Server ç›‘æ§æŠ¥å‘Š</h1>
            <div class="timestamp">{timestamp}</div>
            <div class="status {status}">æ€»ä½“çŠ¶æ€: {status}</div>
        </div>

        <div class="content">
            <!-- æ€»ä½“åˆ†æ•° -->
            <div class="section">
                <h2>ğŸ“Š æ€»ä½“è¯„ä¼°</h2>
                <div class="metrics-grid">
                    <div class="metric-card" style="text-align: center;">
                        <h3>ç»¼åˆè¯„åˆ†</h3>
                        <div class="score-circle">
                            <div class="score-text">{overall_score:.0f}</div>
                        </div>
                        <div class="metric-label">æ»¡åˆ†100åˆ†</div>
                    </div>
                    <div class="metric-card">
                        <h3>å…³é”®æŒ‡æ ‡</h3>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 10px;">
                            <div>
                                <div class="metric-value" style="font-size: 1.2em;">{key_coverage:.1f}%</div>
                                <div class="metric-label">æµ‹è¯•è¦†ç›–ç‡</div>
                            </div>
                            <div>
                                <div class="metric-value" style="font-size: 1.2em;">{key_tests}</div>
                                <div class="metric-label">æµ‹è¯•ç”¨ä¾‹æ•°</div>
                            </div>
                            <div>
                                <div class="metric-value" style="font-size: 1.2em;">{key_success:.1f}%</div>
                                <div class="metric-label">æˆåŠŸç‡</div>
                            </div>
                            <div>
                                <div class="metric-value" style="font-size: 1.2em;">{key_quality:.0f}</div>
                                <div class="metric-label">ä»£ç è´¨é‡</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- è¦†ç›–ç‡åˆ†æ -->
            <div class="section">
                <h2>ğŸ“ˆ æµ‹è¯•è¦†ç›–ç‡åˆ†æ</h2>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <h3>æ€»è¦†ç›–ç‡</h3>
                        <div class="metric-value {coverage_class}">{total_coverage:.1f}%</div>
                        <div class="progress-bar">
                            <div class="progress-fill" style="width: {total_coverage}%"></div>
                        </div>
                        <div class="metric-label">ç›®æ ‡: {threshold}%</div>
                    </div>
                    <div class="metric-card">
                        <h3>è¦†ç›–ç‡åˆ†å¸ƒ</h3>
                        <div style="margin-top: 15px;">
                            {file_coverage_summary}
                        </div>
                    </div>
                </div>
                {uncovered_files_section}
            </div>

            <!-- æ€§èƒ½åˆ†æ -->
            <div class="section">
                <h2>âš¡ æ€§èƒ½åˆ†æ</h2>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <h3>æ‰§è¡Œæ—¶é—´</h3>
                        <div class="metric-value {performance_class}">{total_time:.1f}s</div>
                        <div class="metric-label">å…± {test_count} ä¸ªæµ‹è¯•ç”¨ä¾‹</div>
                    </div>
                    <div class="metric-card">
                        <h3>æˆåŠŸç‡</h3>
                        <div class="metric-value {success_class}">{success_rate:.1f}%</div>
                        <div class="progress-bar">
                            <div class="progress-fill" style="width: {success_rate}%"></div>
                        </div>
                    </div>
                    <div class="metric-card">
                        <h3>å†…å­˜ä½¿ç”¨</h3>
                        <div class="metric-value {memory_class}">{memory_usage:.1f}MB</div>
                        <div class="metric-label">å³°å€¼å†…å­˜å ç”¨</div>
                    </div>
                    <div class="metric-card">
                        <h3>æœ€æ…¢æµ‹è¯•</h3>
                        {slowest_tests_list}
                    </div>
                </div>
            </div>

            <!-- è´¨é‡åˆ†æ -->
            <div class="section">
                <h2>ğŸ¯ ä»£ç è´¨é‡åˆ†æ</h2>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <h3>è´¨é‡åˆ†æ•°</h3>
                        <div class="metric-value {quality_class}">{quality_score:.0f}</div>
                        <div class="metric-label">åŸºäºè¦†ç›–ç‡ã€ç¨³å®šæ€§ç­‰ç»¼åˆè¯„ä¼°</div>
                    </div>
                    <div class="metric-card">
                        <h3>æµ‹è¯•ç¨³å®šæ€§</h3>
                        <div class="metric-value {stability_class}">{test_stability:.1f}%</div>
                        <div class="metric-label">å¤šæ¬¡è¿è¡Œç»“æœä¸€è‡´æ€§</div>
                    </div>
                    <div class="metric-card">
                        <h3>æµ‹è¯•å¤æ‚åº¦</h3>
                        <div class="metric-value {complexity_class}">{test_complexity:.0f}</div>
                        <div class="metric-label">æµ‹è¯•ç”¨ä¾‹å¤æ‚åº¦è¯„åˆ†</div>
                    </div>
                    <div class="metric-card">
                        <h3>é‡å¤æ£€æµ‹</h3>
                        <div class="metric-value {duplicate_class}">{duplicate_coverage:.1f}%</div>
                        <div class="metric-label">é‡å¤æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹</div>
                    </div>
                </div>
            </div>

            <!-- å»ºè®®å’Œæ”¹è¿› -->
            <div class="section">
                <div class="recommendations">
                    <h3>ğŸ’¡ æ”¹è¿›å»ºè®®</h3>
                    {recommendations_html}
                </div>
            </div>
        </div>

        <div class="footer">
            <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp} | Excel MCP Server ç›‘æ§ç³»ç»Ÿ</p>
        </div>
    </div>
</body>
</html>
        """

        # å‡†å¤‡æ•°æ®
        overall_score = report.summary['overall_score']
        status = report.summary['status']
        key_metrics = report.summary['key_metrics']

        # è¦†ç›–ç‡æ•°æ®
        coverage = report.coverage
        coverage_class = "good" if coverage.total_coverage >= self.coverage_threshold else "bad"

        # æ–‡ä»¶è¦†ç›–ç‡æ‘˜è¦
        file_coverage_summary = ""
        if coverage.file_coverage:
            sorted_files = sorted(coverage.file_coverage.items(), key=lambda x: x[1], reverse=True)[:5]
            file_coverage_summary = "<ul>"
            for filename, cov in sorted_files:
                file_coverage_summary += f"<li>{Path(filename).name}: {cov:.1f}%</li>"
            file_coverage_summary += "</ul>"
        else:
            file_coverage_summary = "<p>æ— è¦†ç›–ç‡æ•°æ®</p>"

        # æœªè¦†ç›–æ–‡ä»¶
        uncovered_files_section = ""
        if coverage.uncovered_files:
            uncovered_files_section = f"""
            <div style="margin-top: 20px;">
                <h4>âš ï¸ æœªè¦†ç›–çš„æ–‡ä»¶:</h4>
                <ul>
                    {"".join([f"<li>{file}</li>" for file in coverage.uncovered_files[:5]])}
                </ul>
            </div>
            """

        # æ€§èƒ½æ•°æ®
        performance = report.performance
        performance_class = "good" if performance.total_time < 180 else ("warning" if performance.total_time < 300 else "bad")
        success_class = "good" if performance.success_rate >= 95 else ("warning" if performance.success_rate >= 90 else "bad")
        memory_class = "good" if performance.memory_usage_mb < 500 else ("warning" if performance.memory_usage_mb < 1000 else "bad")

        # æœ€æ…¢æµ‹è¯•åˆ—è¡¨
        slowest_tests_list = ""
        if performance.slowest_tests:
            slowest_tests_list = "<ul>"
            for test_name, duration in performance.slowest_tests[:3]:
                slowest_tests_list += f"<li>{Path(test_name).name}: {duration:.1f}s</li>"
            slowest_tests_list += "</ul>"
        else:
            slowest_tests_list = "<p>æ— æ•°æ®</p>"

        # è´¨é‡æ•°æ®
        quality = report.quality
        quality_class = "good" if quality.code_quality_score >= 80 else ("warning" if quality.code_quality_score >= 60 else "bad")
        stability_class = "good" if quality.test_stability >= 90 else ("warning" if quality.test_stability >= 80 else "bad")
        complexity_class = "good" if quality.test_complexity_score >= 70 else ("warning" if quality.test_complexity_score >= 50 else "bad")
        duplicate_class = "good" if quality.duplicate_coverage < 10 else ("warning" if quality.duplicate_coverage < 20 else "bad")

        # å»ºè®®HTML
        recommendations_html = "<ul>"
        for rec in report.recommendations:
            recommendations_html += f"<li>{rec}</li>"
        recommendations_html += "</ul>"

        return html_template.format(
            timestamp=report.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            status=status,
            overall_score=overall_score,
            score=int(overall_score * 3.6),  # è½¬æ¢ä¸ºè§’åº¦
            key_coverage=key_metrics['coverage'],
            key_tests=key_metrics['test_count'],
            key_success=key_metrics['success_rate'],
            key_quality=key_metrics['quality_score'],
            threshold=self.coverage_threshold,
            total_coverage=coverage.total_coverage,
            coverage_class=coverage_class,
            file_coverage_summary=file_coverage_summary,
            uncovered_files_section=uncovered_files_section,
            total_time=performance.total_time,
            test_count=performance.test_count,
            success_rate=performance.success_rate,
            performance_class=performance_class,
            success_class=success_class,
            memory_usage=performance.memory_usage_mb,
            memory_class=memory_class,
            slowest_tests_list=slowest_tests_list,
            quality_score=quality.code_quality_score,
            quality_class=quality_class,
            test_stability=quality.test_stability,
            stability_class=stability_class,
            test_complexity=quality.test_complexity,
            complexity_class=complexity_class,
            duplicate_coverage=quality.duplicate_coverage,
            duplicate_class=duplicate_class,
            recommendations_html=recommendations_html
        )

    def _generate_text_report(self, report: MonitoringReport) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        text = f"""
{'='*80}
Excel MCP Server ç›‘æ§æŠ¥å‘Š
{'='*80}
ç”Ÿæˆæ—¶é—´: {report.timestamp.strftime("%Y-%m-%d %H:%M:%S")}
æ€»ä½“çŠ¶æ€: {report.summary['status']} (è¯„åˆ†: {report.summary['overall_score']:.1f}/100)

ğŸ“Š å…³é”®æŒ‡æ ‡:
- æµ‹è¯•è¦†ç›–ç‡: {report.coverage.total_coverage:.1f}%
- æµ‹è¯•ç”¨ä¾‹æ•°: {report.performance.test_count}
- æˆåŠŸç‡: {report.performance.success_rate:.1f}%
- å†…å­˜å³°å€¼: {report.performance.memory_usage_mb:.1f}MB
- ä»£ç è´¨é‡: {report.quality.code_quality_score:.0f}

ğŸ“ˆ è¦†ç›–ç‡åˆ†æ:
- æ€»è¦†ç›–ç‡: {report.coverage.total_coverage:.1f}% (ç›®æ ‡: {self.coverage_threshold}%)
- å·²è¦†ç›–æ–‡ä»¶: {len(report.coverage.file_coverage)}
- æœªè¦†ç›–æ–‡ä»¶: {len(report.coverage.uncovered_files)}
"""

        if report.coverage.uncovered_files:
            text += f"  æœªè¦†ç›–æ–‡ä»¶: {', '.join(report.coverage.uncovered_files[:5])}\n"

        text += f"""
âš¡ æ€§èƒ½åˆ†æ:
- æ‰§è¡Œæ—¶é—´: {report.performance.total_time:.1f}s
- æˆåŠŸç‡: {report.performance.success_rate:.1f}%
- å†…å­˜ä½¿ç”¨: {report.performance.memory_usage_mb:.1f}MB (å³°å€¼)
"""

        if report.performance.slowest_tests:
            text += "  æœ€æ…¢çš„æµ‹è¯•:\n"
            for test_name, duration in report.performance.slowest_tests[:3]:
                text += f"    - {test_name}: {duration:.1f}s\n"

        text += f"""
ğŸ¯ è´¨é‡åˆ†æ:
- ä»£ç è´¨é‡åˆ†æ•°: {report.quality.code_quality_score:.0f}/100
- æµ‹è¯•ç¨³å®šæ€§: {report.quality.test_stability:.1f}%
- æµ‹è¯•å¤æ‚åº¦: {report.quality.test_complexity:.0f}/100
- é‡å¤æµ‹è¯•: {report.quality.duplicate_coverage:.1f}%
"""

        if report.quality.flaky_tests:
            text += f"  ä¸ç¨³å®šæµ‹è¯•: {', '.join(report.quality.flaky_tests[:3])}\n"

        text += "\nğŸ’¡ æ”¹è¿›å»ºè®®:\n"
        for i, rec in enumerate(report.recommendations, 1):
            text += f"{i}. {rec}\n"

        text += f"\n{'='*80}\n"

        return text

    def run_full_monitoring(self, report_file: str = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ç›‘æ§"""
        logger.info("å¼€å§‹å®Œæ•´ç›‘æ§æµç¨‹...")

        metrics = {}

        try:
            # 1. è¦†ç›–ç‡ç›‘æ§
            coverage_metrics = self.run_coverage_monitoring()
            metrics['coverage'] = coverage_metrics

            # 2. æ€§èƒ½ç›‘æ§ (åŒ…å«å†…å­˜ç›‘æ§)
            memory_monitor = MemoryMonitor()
            performance_metrics = self.run_performance_monitoring(memory_monitor)
            metrics['performance'] = performance_metrics
            metrics['memory'] = memory_monitor.stop_monitoring()

            # 3. è´¨é‡è¯„ä¼°
            quality_metrics = self.run_quality_assessment(coverage_metrics, performance_metrics)
            metrics['quality'] = quality_metrics

            # 4. ç”Ÿæˆå»ºè®®
            temp_report = MonitoringReport(
                timestamp=datetime.now(),
                coverage=coverage_metrics,
                performance=performance_metrics,
                memory=metrics['memory'],
                quality=quality_metrics,
                recommendations=[],
                summary={}
            )
            recommendations = self.generate_recommendations(temp_report)
            metrics['recommendations'] = recommendations

            # 5. ç”ŸæˆæŠ¥å‘Š
            if not report_file:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                report_file = self.reports_dir / f"monitoring-report-{timestamp}.html"

            report_text = self.generate_report(metrics, str(report_file))

            logger.info("ç›‘æ§æµç¨‹å®Œæˆ")
            return {
                'success': True,
                'metrics': metrics,
                'report_file': str(report_file),
                'text_report': report_text
            }

        except Exception as e:
            logger.error(f"ç›‘æ§æµç¨‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'metrics': metrics
            }

    def run_continuous_monitoring(self, interval_minutes: int = 5):
        """è¿è¡Œè¿ç»­ç›‘æ§"""
        logger.info(f"å¯åŠ¨è¿ç»­ç›‘æ§æ¨¡å¼ï¼Œé—´éš”: {interval_minutes} åˆ†é’Ÿ")

        self._continuous_mode = True

        def signal_handler(signum, frame):
            logger.info("æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            self._continuous_mode = False
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        while self._continuous_mode:
            try:
                logger.info("å¼€å§‹æ–°ä¸€è½®ç›‘æ§...")
                result = self.run_full_monitoring()

                if result['success']:
                    metrics = result['metrics']
                    summary = metrics.get('summary', {})

                    # è¾“å‡ºåˆ°æ§åˆ¶å°
                    print(f"\n{datetime.now().strftime('%H:%M:%S')} - "
                          f"è¦†ç›–ç‡: {metrics['coverage'].total_coverage:.1f}%, "
                          f"æˆåŠŸç‡: {metrics['performance'].success_rate:.1f}%, "
                          f"å†…å­˜: {metrics['performance'].memory_usage_mb:.0f}MB, "
                          f"è´¨é‡: {metrics['quality'].code_quality_score:.0f}")
                else:
                    logger.error(f"ç›‘æ§å¤±è´¥: {result.get('error', 'Unknown error')}")

                # ç­‰å¾…ä¸‹ä¸€æ¬¡ç›‘æ§
                if self._continuous_mode:
                    time.sleep(interval_minutes * 60)

            except KeyboardInterrupt:
                logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºè¿ç»­ç›‘æ§")
                break
            except Exception as e:
                logger.error(f"è¿ç»­ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(interval_minutes * 60)

        self._continuous_mode = False
        logger.info("è¿ç»­ç›‘æ§å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Excel MCP Server ç›‘æ§å’Œç»´æŠ¤è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    python scripts/monitor-and-maintain.py
    python scripts/monitor-and-maintain.py --coverage-only
    python scripts/monitor-and-maintain.py --continuous --threshold 90
    python scripts/monitor-and-maintain.py --no-html --report-file report.txt
        """
    )

    parser.add_argument(
        "--coverage-only",
        action="store_true",
        help="ä»…è¿è¡Œè¦†ç›–ç‡ç›‘æ§"
    )

    parser.add_argument(
        "--performance-only",
        action="store_true",
        help="ä»…è¿è¡Œæ€§èƒ½ç›‘æ§"
    )

    parser.add_argument(
        "--memory-only",
        action="store_true",
        help="ä»…è¿è¡Œå†…å­˜ç›‘æ§"
    )

    parser.add_argument(
        "--quality-only",
        action="store_true",
        help="ä»…è¿è¡Œè´¨é‡è¯„ä¼°"
    )

    parser.add_argument(
        "--report-file",
        default=None,
        help="æŒ‡å®šæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: reports/monitoring-report.html)"
    )

    parser.add_argument(
        "--threshold",
        type=float,
        default=85.0,
        help="è¦†ç›–ç‡é˜ˆå€¼ (é»˜è®¤: 85)"
    )

    parser.add_argument(
        "--no-html",
        action="store_true",
        help="ä¸ç”ŸæˆHTMLæŠ¥å‘Šï¼Œä»…è¾“å‡ºæ–‡æœ¬"
    )

    parser.add_argument(
        "--continuous",
        action="store_true",
        help="è¿ç»­ç›‘æ§æ¨¡å¼ï¼Œæ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡"
    )

    parser.add_argument(
        "--interval",
        type=int,
        default=5,
        help="è¿ç»­ç›‘æ§é—´éš”(åˆ†é’Ÿ) (é»˜è®¤: 5)"
    )

    args = parser.parse_args()

    # åˆ›å»ºç›‘æ§å™¨
    monitor = TestMonitor(coverage_threshold=args.threshold)

    try:
        if args.continuous:
            # è¿ç»­ç›‘æ§æ¨¡å¼
            monitor.run_continuous_monitoring(args.interval)
        elif args.coverage_only:
            # ä»…è¦†ç›–ç‡ç›‘æ§
            metrics = monitor.run_coverage_monitoring()
            print(f"æµ‹è¯•è¦†ç›–ç‡: {metrics.total_coverage:.1f}%")
            if metrics.uncovered_files:
                print(f"æœªè¦†ç›–æ–‡ä»¶: {', '.join(metrics.uncovered_files)}")
        elif args.performance_only:
            # ä»…æ€§èƒ½ç›‘æ§
            memory_monitor = MemoryMonitor()
            metrics = monitor.run_performance_monitoring(memory_monitor)
            print(f"æ‰§è¡Œæ—¶é—´: {metrics.total_time:.1f}s")
            print(f"æµ‹è¯•æ•°é‡: {metrics.test_count}")
            print(f"æˆåŠŸç‡: {metrics.success_rate:.1f}%")
            print(f"å†…å­˜ä½¿ç”¨: {metrics.memory_usage_mb:.1f}MB")
        elif args.memory_only:
            # ä»…å†…å­˜ç›‘æ§ (éœ€è¦é…åˆå…¶ä»–ç›‘æ§)
            print("å†…å­˜ç›‘æ§éœ€è¦é…åˆå…¶ä»–ç›‘æ§é€‰é¡¹ä½¿ç”¨")
        elif args.quality_only:
            # ä»…è´¨é‡è¯„ä¼° (éœ€è¦å…ˆè¿è¡Œå…¶ä»–ç›‘æ§è·å–æ•°æ®)
            print("è´¨é‡è¯„ä¼°éœ€è¦å…ˆè¿è¡Œè¦†ç›–ç‡æˆ–æ€§èƒ½ç›‘æ§")
        else:
            # å®Œæ•´ç›‘æ§
            result = monitor.run_full_monitoring(args.report_file)

            if result['success']:
                print("\n" + "="*60)
                print("ç›‘æ§å®Œæˆï¼")
                print("="*60)

                if not args.no_html and result.get('report_file'):
                    print(f"ğŸ“„ HTMLæŠ¥å‘Š: {result['report_file']}")

                # è¾“å‡ºç®€è¦ä¿¡æ¯
                metrics = result['metrics']
                print(f"ğŸ“Š è¦†ç›–ç‡: {metrics['coverage'].total_coverage:.1f}%")
                print(f"âš¡ æ‰§è¡Œæ—¶é—´: {metrics['performance'].total_time:.1f}s")
                print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {metrics['performance'].memory_usage_mb:.1f}MB")
                print(f"ğŸ¯ è´¨é‡åˆ†æ•°: {metrics['quality'].code_quality_score:.0f}")
                print(f"âœ… å»ºè®®: {len(metrics['recommendations'])} æ¡")
            else:
                print(f"âŒ ç›‘æ§å¤±è´¥: {result.get('error', 'Unknown error')}")
                return 1

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 0
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())